---
title: 'Querying Text Models'
description: 'Learn how to interact with Diffusive Labs text models'
---

# Querying Text Models

Diffusive Labs offers an OpenAI-compatible REST API for querying text models. You can interact with our models through:

* The Diffusive Python client library
* The web console
* LangChain
* Direct REST API calls using any programming language
* The OpenAI Python client

## Using the Web Console

All Diffusive models can be accessed through our web console at diffusive.ai. The playground allows you to test models by entering prompts and adjusting request parameters.

* **Non-chat models** use the completions API, passing your input directly to the model
* **Chat models** (also known as instruct models) use the chat completions API by default, automatically formatting your input with the model's conversation style
* Advanced users can switch to the completions API by disabling "Use chat template"

## Using the API

### Chat Completions API

Models with conversation configurations use the chat completions API. These models are optimized for specific conversation styles. For example, our Llama chat models use this template:
text
<s>[INST] <<SYS>>
<</SYS>>
{user_message_1} [/INST]


We recommend using the chat completions API when possible to avoid prompt formatting errors.

#### Overriding the System Prompt

Many models include default system prompts. For example, our Llama 2 models use:

> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.

You can override this by setting the first message with role `system`:
json
[
{
"role": "system",
"content": "You are a pirate."
},
{
"role": "user",
"content": "Hello, what is your name?"
}
]


### Completions API

The completions API is supported by all text models. It generates text based on your input prompt until reaching either:
* Maximum output tokens
* Model's end-of-sequence (EOS) token

Most models automatically prepend the beginning-of-sequence (BOS) token to your prompt.

## Getting Usage Info

All responses include a `usage` field containing:
* Number of prompt tokens processed
* Number of completion tokens generated

## Advanced Options

### Streaming

Enable streaming to receive results incrementally as tokens are generated:

python
from diffusive.client import Diffusive
client = Diffusive(api_key="<DIFFUSIVE_API_KEY>")
response = client.completions.create(
model="accounts/diffusive/models/mixtral-8x7b-instruct",
prompt="Write a story about",
stream=True
)
for chunk in response:
print(chunk.choices[0].text, end="")


### Async Mode

Our Python client supports asynchronous operations:
python
async with Diffusive(api_key="<DIFFUSIVE_API_KEY>") as client:
response = await client.chat.completions.create(
model="accounts/diffusive/models/mixtral-8x7b-instruct",
messages=[{"role": "user", "content": "Hello!"}]
)


### Sampling Options

Control text generation with these parameters:

* `temperature` (0-2): Higher values increase randomness
* `top_p` (0-1): Nucleus sampling probability threshold
* `top_k` (1+): Limits token selection to top K options
* `min_p` (0-1): Minimum token probability threshold
* `frequency_penalty` (-2 to 2): Penalizes frequent tokens
* `presence_penalty` (-2 to 2): Penalizes any repeated tokens
* `repetition_penalty` (1+): Scales penalty for repeated tokens

### Debugging Options

* `ignore_eos`: Continue generation past EOS token
* `logprobs`: Return token probability information
* `echo`: Include prompt in response
* `raw_output`: Return detailed model input/output data (experimental)

## Appendix

### Tokenization

Models process text as tokens, which can be characters, subwords, or words. Different models use different tokenizers, affecting token counts and costs. Check the `usage` field in responses for actual token counts.

<Card title="Token Counter" icon="calculator" href="/tools/token-counter">
  Estimate token counts for your inputs
</Card>